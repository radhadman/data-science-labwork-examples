{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 6: Collaborative Filtering\n",
    "\n",
    "In this lab, we will walk through an implementation of collaborative filtering in Python. After seeing a complete implementation, you will then implement a couple of simpler approaches (called baselines) for comparison. \n",
    "\n",
    "This lab is adapted from the following online tutorial: https://www.ethanrosenthal.com/2015/11/02/intro-to-collaborative-filtering/ \n",
    "\n",
    "We will be using the MovieLens dataset. This is a classic dataset for training recommendation models. There are various datasets, but the one that we will use below consists of 100,000 movie ratings by users (on a 1-5 scale). The main data file consists of a tab-separated list with user-id (starting at 1), item-id (starting at 1), rating, and timestamp as the four fields. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we will read in the dataset and get some basic stats from the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>196</td>\n",
       "      <td>242</td>\n",
       "      <td>3</td>\n",
       "      <td>881250949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>186</td>\n",
       "      <td>302</td>\n",
       "      <td>3</td>\n",
       "      <td>891717742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>22</td>\n",
       "      <td>377</td>\n",
       "      <td>1</td>\n",
       "      <td>878887116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>244</td>\n",
       "      <td>51</td>\n",
       "      <td>2</td>\n",
       "      <td>880606923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>166</td>\n",
       "      <td>346</td>\n",
       "      <td>1</td>\n",
       "      <td>886397596</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  item_id  rating  timestamp\n",
       "0      196      242       3  881250949\n",
       "1      186      302       3  891717742\n",
       "2       22      377       1  878887116\n",
       "3      244       51       2  880606923\n",
       "4      166      346       1  886397596"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names = ['user_id', 'item_id', 'rating', 'timestamp']\n",
    "df = pd.read_csv('u.data', sep='\\t', names=names)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "943 users\n",
      "1682 items\n"
     ]
    }
   ],
   "source": [
    "n_users = df.user_id.unique().shape[0]\n",
    "n_items = df.item_id.unique().shape[0]\n",
    "print (str(n_users) + ' users')\n",
    "print (str(n_items) + ' items')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most recommendation models consist of building a user-by-item matrix with some sort of \"interaction\" number in each cell. If one includes the numerical ratings that users give items, then this is called an *explicit feedback* model. Alternatively, one may include *implicit feedback* which are actions by a user that signify a positive or negative preference for a given item (such as viewing the item online). These two scenarios often must be treated differently.\n",
    "\n",
    "In the case of the MovieLens dataset, we have ratings, so we will focus on explicit feedback models. First, we must construct our user-item matrix. We can easily map user/item ID's to user/item indices by removing the \"Python starts at 0\" offset between them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 5.,  3.,  4., ...,  0.,  0.,  0.],\n",
       "       [ 4.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       ..., \n",
       "       [ 5.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  5.,  0., ...,  0.,  0.,  0.]])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings = np.zeros((n_users, n_items))\n",
    "for row in df.itertuples():\n",
    "    ratings[row[1]-1, row[2]-1] = row[3]\n",
    "ratings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this dataset, every user has rated at least 20 movies which results in a reasonable sparsity of 6.3%. This means that 6.3% of the user-item ratings have a value. Note that, although we filled in missing ratings as 0, we should not assume these values to truly be zero. More appropriately, they are just empty entries. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will split our data into training and test sets by removing 10 ratings per user from the training set and placing them in the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_test_split(ratings):\n",
    "    test = np.zeros(ratings.shape)\n",
    "    train = ratings.copy()\n",
    "    for user in range(ratings.shape[0]):\n",
    "        test_ratings = np.random.choice(ratings[user, :].nonzero()[0], \n",
    "                                        size=10, \n",
    "                                        replace=False)\n",
    "        train[user, test_ratings] = 0.\n",
    "        test[user, test_ratings] = ratings[user, test_ratings]\n",
    "        \n",
    "    # Test and training are truly disjoint\n",
    "    assert(np.all((train * test) == 0)) \n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train, test = train_test_split(ratings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collaborative filtering\n",
    "We will focus on collaborative filtering models today which can be generally split into two classes: user- and item-based collaborative filtering. In either scenario, one builds a similarity matrix. For user-based collaborative filtering, the user-similarity matrix will consist of some distance metric that measures the similarity between any two pairs of users. Likewise, the item-similarity matrix will measure the similarity between any two pairs of items. \n",
    "\n",
    "__Note:__ The original tutorial covers both user-based and item-based implementations, but in this lab we focus on just the user-based approach.\n",
    "\n",
    "A common distance metric is cosine similarity. The metric can be thought of geometrically if one treats a given user's (item's) row (column) of the ratings matrix as a vector. For user-based collaborative filtering, two users' similarity is measured as the cosine of the angle between the two users' vectors. For users ${u}$ and ${u'}$, the cosine similarity is\n",
    "\n",
    "$$\n",
    "sim(u, u') = \n",
    "cos(\\theta{}) = \n",
    "\\frac{\\textbf{r}_{u} \\dot{} \\textbf{r}_{u'}}{\\| \\textbf{r}_{u} \\| \\| \\textbf{r}_{u'} \\|} = \n",
    "\\sum_{i} \\frac{r_{ui}r_{u'i}}{\\sqrt{\\sum\\limits_{i} r_{ui}^2} \\sqrt{\\sum\\limits_{i} r_{u'i}^2} }\n",
    "$$\n",
    "\n",
    "This can be written as a for-loop with code, but the Python code will run quite slow; instead, one should try to express any equation in terms of NumPy functions. I've included a slow and fast version of the cosine similarity function below. The slow function took so long that I eventually canceled it because I got tired of waiting. The fast function, on the other hand, takes around 200 ms. \n",
    "\n",
    "The cosine similarity will range from 0 to 1 in our case (because there are no negative ratings). Notice that it is symmetric and has ones along the diagonal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def slow_similarity(ratings):\n",
    "    axmax = 0\n",
    "    axmin = 1\n",
    "\n",
    "    sim = np.zeros((ratings.shape[axmax], ratings.shape[axmax]))\n",
    "    for u in range(ratings.shape[axmax]):\n",
    "        for uprime in range(ratings.shape[axmax]):\n",
    "            rui_sqrd = 0.\n",
    "            ruprimei_sqrd = 0.\n",
    "            for i in range(ratings.shape[axmin]):\n",
    "                sim[u, uprime] = ratings[u, i] * ratings[uprime, i]\n",
    "                rui_sqrd += ratings[u, i] ** 2\n",
    "                ruprimei_sqrd += ratings[uprime, i] ** 2\n",
    "            sim[u, uprime] /= rui_sqrd * ruprimei_sqrd\n",
    "    return sim\n",
    "\n",
    "def fast_similarity(ratings, epsilon=1e-9):\n",
    "    # epsilon -> small number for handling dived-by-zero errors\n",
    "    sim = ratings.dot(ratings.T) + epsilon   \n",
    "    norms = np.array([np.sqrt(np.diagonal(sim))])\n",
    "    return (sim / norms / norms.T)\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#%timeit slow_user_similarity(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83.4 ms ± 11.5 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit fast_similarity(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.          0.12757633  0.0396659   0.06130388]\n",
      " [ 0.12757633  1.          0.11743494  0.21153424]\n",
      " [ 0.0396659   0.11743494  1.          0.26018616]\n",
      " [ 0.06130388  0.21153424  0.26018616  1.        ]]\n"
     ]
    }
   ],
   "source": [
    "user_similarity = fast_similarity(train)\n",
    "print(user_similarity[:4, :4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our similarity matrix in hand, we can now predict the ratings that were not included with the data. Using these predictions, we can then compare them with the test data to attempt to validate the quality of our recommender model.\n",
    "\n",
    "For user-based collaborative filtering, we predict that a user's $u$'s rating for item $i$ is given by the weighted sum of all other users' ratings for item $i$ where the weighting is the cosine similarity between the each user and the input user $u$.\n",
    "\n",
    "$$\\hat{r}_{ui} = \\sum\\limits_{u'}sim(u, u') r_{u'i}$$\n",
    "\n",
    "We must also normalize by the number of ${r_{u'i}}$ ratings:\n",
    "\n",
    "$$\\hat{r}_{ui} = \\frac{\\sum\\limits_{u'} sim(u, u') r_{u'i}}{\\sum\\limits_{u'}|sim(u, u')|}$$\n",
    "\n",
    "As with before, our computational speed will benefit greatly by favoring NumPy functions over for loops. With our slow function below, even though I use NumPy methods, the presence of the for-loop still slows the algorithm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict_slow_simple(ratings, similarity):\n",
    "    pred = np.zeros(ratings.shape)\n",
    "\n",
    "    for i in range(ratings.shape[0]):\n",
    "        for j in range(ratings.shape[1]):\n",
    "            pred[i, j] = similarity[i, :].dot(ratings[:, j])\\\n",
    "                         /np.sum(np.abs(similarity[i, :]))\n",
    "    return pred\n",
    "\n",
    "def predict_fast_simple(ratings, similarity):\n",
    "    return similarity.dot(ratings) / np.array([np.abs(similarity).sum(axis=1)]).T\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#%timeit predict_slow_simple(train, user_similarity, kind='user')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "124 ms ± 3.8 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit predict_fast_simple(train, user_similarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use the scikit-learn's mean squared error function as our validation metric. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def get_mse(pred, actual):\n",
    "    # Ignore nonzero terms.\n",
    "    pred = pred[actual.nonzero()].flatten()\n",
    "    actual = actual[actual.nonzero()].flatten()\n",
    "    return mean_squared_error(pred, actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User-based CF MSE: 8.38595260368\n"
     ]
    }
   ],
   "source": [
    "user_prediction = predict_fast_simple(train, user_similarity)\n",
    "\n",
    "print('User-based CF MSE: ' + str(get_mse(user_prediction, test)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see an MSE score of around 8.42. __Note:__ This will vary slightly on each run because of randomness in how the train/test split is created."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab Assignment\n",
    "\n",
    "For the lab assignment, you need to complete a few additional steps that will help us understand the performance of the collaborative filtering system. If we just see a score of 8.42, it is hard to know if that is bad or good. You will implement a couple of simple baseline models for comparison. A baseline is normally something that is very simple to implement but which might be expected to work well. Some baselines can be surprisingly hard to beat on certain tasks. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Baseline 1__: For this baseline, you just need to calculate what the MSE would be if we did not do anything. That is, what would the MSE be if we just left the zeroes in the training set and compared with the test set? This should be very easy to implement, and if you implement it correctly you should see an MSE around 14.2. __Note:__ The MSE will vary on each run because there is randomness in how the train/test split is determined.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User-based CF MSE: 14.1205726405\n"
     ]
    }
   ],
   "source": [
    "# Your Baseline 1 code here\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def get_mse(pred, actual):\n",
    "    pred = pred[actual.nonzero()].flatten()\n",
    "    actual = actual[actual.nonzero()].flatten()\n",
    "    return mean_squared_error(pred, actual)\n",
    "    \n",
    "\n",
    "user_prediction = predict_fast_simple(train, user_similarity)\n",
    "\n",
    "print('User-based CF MSE: ' + str(get_mse(train, test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next baselines are also simple, but will take a few more lines of code to implement. They use something called _median imputation_. Basically, for every 0 in the training set, we will replace it with the median (central) value. There are two ways to do this. If a particular user/movie combo in the dataset has a 0, we could replace it with either:\n",
    "* the median value of all the user's ratings\n",
    "* the median value of all the movie's ratings\n",
    "\n",
    "You will implement both of those approaches. One of them calculates a median across rows, and the other calculates a median across columns. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas has some useful built-in functions for doing median imputation (and other types of imputation), so your best bet is to convert the training data to a Pandas DataFrame. And those Pandas functions are designed to replace 'NaN' values, so the zeroes need to be converted to 'NaN' values. \n",
    "\n",
    "We will walk through a __toy example__ below that you can use as the basis of your implementations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 2]\n",
      " [1 0 3]\n",
      " [0 2 2]\n",
      " [3 0 0]]\n"
     ]
    }
   ],
   "source": [
    "# some toy data, with 0 (missing) values\n",
    "X = np.array([[0, 1, 2], [1,0,3], [0, 2, 2], [3,0,0]])        \n",
    "print(X)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.5  1.   2. ]\n",
      " [ 1.   2.   3. ]\n",
      " [ 1.5  2.   2. ]\n",
      " [ 3.   2.   2. ]]\n"
     ]
    }
   ],
   "source": [
    "# convert to Pandas DataFrame\n",
    "dat = pd.DataFrame(X)\n",
    "# replace zeroes with 'NaN' values\n",
    "dat = dat.replace(0, np.nan)\n",
    "# use the fillna() function to replace 'NaN' values with the median (over rows).\n",
    "# if axis=0, then it calculates the median by columns. \n",
    "dat = dat.fillna(dat.median(axis=1))\n",
    "\n",
    "# if there is some column (or row) that has only 'NaN' values,\n",
    "# then the median cannot be calculated and nothing will be imputed. \n",
    "# we need to ensure all 'NaN' values are replaced, so we could change \n",
    "# the remaining ones to 0 (or do something else like use the median over the entire dataset). \n",
    "\n",
    "dat = dat.replace(np.nan, 0)\n",
    "\n",
    "# then convert back to a NumPy array\n",
    "impute = dat.values\n",
    "print(impute)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Baseline2 :__ Do median imputation over columns (movies) and calculate the resulting MSE. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 5.  3.  4. ...,  0.  0.  0.]\n",
      " [ 4.  4.  3. ...,  0.  0.  0.]\n",
      " [ 4.  4.  3. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 4.  4.  3. ...,  0.  0.  0.]\n",
      " [ 4.  4.  3. ...,  0.  0.  0.]\n",
      " [ 4.  5.  3. ...,  0.  0.  0.]]\n",
      "User-based CF MSE: 0.249129400157\n"
     ]
    }
   ],
   "source": [
    "# Your Baseline 2 code here\n",
    "dat = pd.DataFrame(train)\n",
    "dat = dat.replace(0, np.nan)\n",
    "dat = dat.fillna(dat.median(axis=1))\n",
    "dat = dat.replace(np.nan, 0)\n",
    "\n",
    "impute = dat.values\n",
    "print(impute)\n",
    "\n",
    "print('User-based CF MSE: ' + str(get_mse(test, user_prediction)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Baseline 3:__ Do median imputation over rows (users) and calculate the resulting MSE. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 5.  3.  4. ...,  2.  3.  3.]\n",
      " [ 4.  3.  3. ...,  2.  3.  3.]\n",
      " [ 4.  3.  3. ...,  2.  3.  3.]\n",
      " ..., \n",
      " [ 4.  3.  3. ...,  2.  3.  3.]\n",
      " [ 4.  3.  3. ...,  2.  3.  3.]\n",
      " [ 4.  5.  3. ...,  2.  3.  3.]]\n",
      "User-based CF MSE: 1.19950620084\n"
     ]
    }
   ],
   "source": [
    "# Your Baseline 3 code here\n",
    "dat = pd.DataFrame(train)\n",
    "dat = dat.replace(0, np.nan)\n",
    "dat = dat.fillna(dat.median(axis=0))\n",
    "dat = dat.replace(np.nan, 0)\n",
    "\n",
    "impute = dat.values\n",
    "print(impute)\n",
    "\n",
    "print('User-based CF MSE: ' + str(get_mse(train, user_similarity)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you implement median imputation correctly, you should see the MSE on the test set is actually considerably lower than when using collaborative filtering. The MSE scores will be around 2.4 and 1.2, respectively, for the median imputation approaches. \n",
    "\n",
    "This does not mean that the collaborative filtering approach is bad. But we always need to analyse systems in the context of baselines and performance bounds. In this case, a simple approach works very well. It might be possible to combine collaborative filtering and median imputation to get even better performance (good project idea). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Optional:__ If you finish the above steps very quickly, try some additional approaches:\n",
    "* Use mean imputation instead of median imputation. What is the MSE?\n",
    "* Try imputing the median or mean over the entire dataset (rather than column-by-column or row-by-row). What is the MSE?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Deliverables:__ Submit your completed notebook via Blackboard."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
