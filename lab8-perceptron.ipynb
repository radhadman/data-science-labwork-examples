{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 8: The Perceptron\n",
    "\n",
    "In this lab, you will complete an implementation of the Perceptron training algorithm. \n",
    "\n",
    "First a synthetic dataset is generated. The code then does k-fold cross-validation on that data. This means the data is split into 5 folds, and we train on 4 of the folds and evaluate on the remaining fold. We do that 5 times so that we evaluate on each of the 5 folds. \n",
    "\n",
    "The code reports precision and recall scores for evaluation. (https://en.wikipedia.org/wiki/Precision_and_recall ) Ideally these should both be close to 1, but because the training algorithm has not been completed, when you initially run this you should see something like Precision=0.5 and Recall=1.0 for each fold. In other words, recall is perfect because everything is being predicted as the positive class, which makes precision very low. \n",
    "\n",
    "Your goal for the lab is to complete the train() function below so that precision and recall are both close to 1. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function for Generating Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function genData() will generate some synthetic data consisting of two features/predictors and a binary response/outcome. The data is randomly generated but will look something like this, where each instance has a label 1 or -1 representing the outcome variable, followed by the values of the two features.\n",
    "\n",
    "(1, array([ 1.49640192,  1.68659547]))\n",
    "\n",
    "(-1, array([ 0.8539312 ,  0.98891425]))\n",
    "\n",
    "…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import sys, os\n",
    "import re, string\n",
    "\n",
    "# Generates training data with two features.\n",
    "# DO NOT MODIFY.\n",
    "def genData(iterations):\n",
    "    dataset = []\n",
    "    for i in range(0,int(iterations/2.0)):\n",
    "        # positive examples\n",
    "        num1 = random.uniform(1.1,1.9)\n",
    "        num2 = random.uniform(1.1,1.9)\n",
    "        dataset.append((1,np.array([num1,num2])))\n",
    "\n",
    "        # negative examples\n",
    "        num1 = random.uniform(0.7,1.3)\n",
    "        num2 = random.uniform(0.7,1.3)\n",
    "        dataset.append((-1,np.array([num1,num2])))\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function for Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function below is the only part of the code that you need to modify. But before you make any changes, try running all of the code and checking the precision and recall. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# COMPLETE THIS FUNCTION.\n",
    "# This function does the online perceptron training. \n",
    "def train(train_data, w):\n",
    "    bias = 0\n",
    "    for i in range(0,10):\n",
    "        random.shuffle(train_data)\n",
    "    \n",
    "        '''\n",
    "        TO COMPLETE:\n",
    "        The outer loop does ten iterations over the dataset, shuffling the data each time.\n",
    "        For each of those ten iterations, have an inner loop consider one data point at a time. \n",
    "        Calculate the activation for the datapoint, and update the weights and bias when there is an error.\n",
    "        Follow the pseudocode.\n",
    " \n",
    "        '''\n",
    "        \n",
    "        for (y,x) in train_data:\n",
    "            activation = w * x\n",
    "            if activation <= 0:\n",
    "                if (y == -1):\n",
    "                    w = w + y*x\n",
    "                    bias += 1\n",
    "                else: \n",
    "                    w += 1\n",
    "            else:\n",
    "                if (y == -1):\n",
    "                    w += 1\n",
    "                else:\n",
    "                    w += 1\n",
    "                    bias += 1 \n",
    "            \n",
    "        \n",
    "    return (w, bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function for Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the model is trained, the following test() function is used to evaluate the model. It takes the learned weights and bias and applies the learned model to some unseen test data. Precision and recall are calculated. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Evaluates the trained model on the held-out portion of data.\n",
    "# DO NOT MODIFY.\n",
    "def test(w, bias, test_data, thresh=0):\n",
    "    incorrect = 0\n",
    "    tp = fp = tn = fn = 0\n",
    "    for (y,x) in test_data:\n",
    "        activation = bias + np.inner(w, x)\n",
    "        if activation >= thresh:\n",
    "            if (y == -1):\n",
    "                fp += 1\n",
    "                incorrect += 1\n",
    "            else:\n",
    "                tp += 1\n",
    "        else:\n",
    "            if (y == -1):\n",
    "                tn += 1\n",
    "            else:\n",
    "                fn += 1\n",
    "                incorrect += 1\n",
    "    accuracy = ((len(test_data)-float(incorrect)) / len(test_data)) * 100\n",
    "    \n",
    "    try:\n",
    "        precision = tp / float(tp + fp)\n",
    "    except:\n",
    "        precision = 0\n",
    "    try:\n",
    "        recall = tp / float(tp + fn)\n",
    "    except:\n",
    "        recall = 0\n",
    "    print('Precision: %s, Recall %s' % (precision, recall))\n",
    "    return (accuracy,precision,recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function for K-Fold Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Does cross-validation.\n",
    "# Splits the data into k folds, trains on k-1 folds and tests on 1 fold, and repeats k times.\n",
    "# DO NOT MODIFY.\n",
    "def cv(data, k=5):\n",
    "    start = 0\n",
    "    fold_size = int(len(data) / float(k))\n",
    "    init_w = 0.0\n",
    "    w = np.array([init_w]*len(data[0][1])) # initialize weights\n",
    "    \n",
    "    for i in range(0,k):\n",
    "        train_data = data[start:start+fold_size]\n",
    "        test_data = data[0:start] + data[start+fold_size:]\n",
    "        (w, bias) = train(train_data, w)\n",
    "        (acc,p,r) = test(w, bias, test_data)\n",
    "        start = start+fold_size \n",
    "    return (w,bias)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Out All the Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following lines of code generate 1500 observations, and run 5-fold cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.5, Recall 1.0\n",
      "Precision: 0.5, Recall 1.0\n",
      "Precision: 0.5, Recall 1.0\n",
      "Precision: 0.5, Recall 1.0\n",
      "Precision: 0.5, Recall 1.0\n"
     ]
    }
   ],
   "source": [
    "data = genData(1500)      # generate some data\n",
    "(w, bias) = cv(data, 5)  # do 5-fold cross-validation on the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deliverable: Submit your completed notebook via Blackboard."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you finish early, try modifying the training method to use the _averaged perceptron_ technique. See this link, page 53, for pseudocode and an explanation: http://ciml.info/dl/v0_99/ciml-v0_99-ch04.pdf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
